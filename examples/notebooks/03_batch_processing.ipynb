{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Batch Processing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/artiso-ai/dppvalidator/blob/main/examples/notebooks/03_batch_processing.ipynb)\n",
    "\n",
    "Learn how to validate multiple DPPs efficiently using async batch processing.\n",
    "\n",
    "This notebook covers:\n",
    "- Async validation for high throughput\n",
    "- Batch validation with concurrency control\n",
    "- Processing directories of DPP files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q dppvalidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Sample Data\n",
    "\n",
    "Let's create multiple DPPs to validate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample DPPs\n",
    "def create_sample_dpp(product_id: int, recycled_content: float) -> dict:\n",
    "    return {\n",
    "        \"type\": [\"DigitalProductPassport\", \"VerifiableCredential\"],\n",
    "        \"@context\": [\n",
    "            \"https://www.w3.org/ns/credentials/v2\",\n",
    "            \"https://test.uncefact.org/vocabulary/untp/dpp/0.6.1/\",\n",
    "        ],\n",
    "        \"id\": f\"https://example.com/credentials/dpp-{product_id:04d}\",\n",
    "        \"issuer\": {\"id\": \"https://example.com/issuers/001\", \"name\": \"Batch Co.\"},\n",
    "        \"credentialSubject\": {\n",
    "            \"type\": [\"Product\"],\n",
    "            \"id\": f\"https://example.com/products/{product_id:04d}\",\n",
    "            \"name\": f\"Product {product_id}\",\n",
    "            \"circularityScorecard\": {\n",
    "                \"type\": [\"CircularityScorecard\"],\n",
    "                \"recycledContent\": recycled_content,\n",
    "                \"recyclableContent\": 0.90,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Create 100 sample DPPs with varying recycled content\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "sample_dpps = [create_sample_dpp(i, random.uniform(0.1, 0.8)) for i in range(100)]\n",
    "\n",
    "print(f\"Created {len(sample_dpps)} sample DPPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Synchronous Batch Validation\n",
    "\n",
    "Simple approach - validate one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from dppvalidator.validators import ValidationEngine\n",
    "\n",
    "engine = ValidationEngine()\n",
    "\n",
    "# Synchronous validation\n",
    "start = time.perf_counter()\n",
    "sync_results = [engine.validate(dpp) for dpp in sample_dpps]\n",
    "sync_time = time.perf_counter() - start\n",
    "\n",
    "valid_count = sum(1 for r in sync_results if r.valid)\n",
    "print(f\"Sync validation: {len(sample_dpps)} DPPs in {sync_time:.3f}s\")\n",
    "print(f\"Valid: {valid_count}/{len(sample_dpps)}\")\n",
    "print(f\"Throughput: {len(sample_dpps) / sync_time:.0f} DPPs/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Async Batch Validation\n",
    "\n",
    "Use `validate_batch` for concurrent validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def run_batch_validation():\n",
    "    start = time.perf_counter()\n",
    "    results = await engine.validate_batch(sample_dpps, concurrency=10)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    return results, elapsed\n",
    "\n",
    "\n",
    "# Run async batch validation\n",
    "async_results, async_time = await run_batch_validation()\n",
    "\n",
    "valid_count = sum(1 for r in async_results if r.valid)\n",
    "print(f\"Async validation: {len(sample_dpps)} DPPs in {async_time:.3f}s\")\n",
    "print(f\"Valid: {valid_count}/{len(sample_dpps)}\")\n",
    "print(f\"Throughput: {len(sample_dpps) / async_time:.0f} DPPs/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽšï¸ Concurrency Tuning\n",
    "\n",
    "Test different concurrency levels to find optimal throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def benchmark_concurrency(concurrency: int):\n",
    "    start = time.perf_counter()\n",
    "    results = await engine.validate_batch(sample_dpps, concurrency=concurrency)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    throughput = len(sample_dpps) / elapsed\n",
    "    return concurrency, elapsed, throughput\n",
    "\n",
    "\n",
    "# Test different concurrency levels\n",
    "print(\"Concurrency | Time (s) | Throughput (DPPs/s)\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for concurrency in [1, 5, 10, 20, 50]:\n",
    "    c, t, tp = await benchmark_concurrency(concurrency)\n",
    "    print(f\"{c:^11} | {t:^8.3f} | {tp:^18.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Processing Files from Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temp directory with DPP files\n",
    "temp_dir = Path(\"temp_dpps\")\n",
    "temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save first 10 DPPs as files\n",
    "for i, dpp in enumerate(sample_dpps[:10]):\n",
    "    (temp_dir / f\"dpp_{i:04d}.json\").write_text(json.dumps(dpp, indent=2))\n",
    "\n",
    "\n",
    "# Validate all files in directory\n",
    "def validate_directory(directory: Path) -> list:\n",
    "    results = []\n",
    "    for json_file in directory.glob(\"*.json\"):\n",
    "        result = engine.validate_file(json_file)\n",
    "        results.append((json_file.name, result))\n",
    "    return results\n",
    "\n",
    "\n",
    "file_results = validate_directory(temp_dir)\n",
    "print(f\"Validated {len(file_results)} files:\")\n",
    "for filename, result in file_results:\n",
    "    status = \"âœ“\" if result.valid else \"âœ—\"\n",
    "    print(f\"  {status} {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Aggregating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Aggregate error codes across all results\n",
    "all_errors = []\n",
    "all_warnings = []\n",
    "\n",
    "for result in sync_results:\n",
    "    all_errors.extend([e.code for e in result.errors])\n",
    "    all_warnings.extend([w.code for w in result.warnings])\n",
    "\n",
    "error_counts = Counter(all_errors)\n",
    "warning_counts = Counter(all_warnings)\n",
    "\n",
    "print(\"Error Code Distribution:\")\n",
    "for code, count in error_counts.most_common(5):\n",
    "    print(f\"  {code}: {count}\")\n",
    "\n",
    "print(\"\\nWarning Code Distribution:\")\n",
    "for code, count in warning_counts.most_common(5):\n",
    "    print(f\"  {code}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "print(\"âœ“ Cleaned up temporary files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Key Takeaways\n",
    "\n",
    "- Use `validate_batch()` for high-throughput scenarios\n",
    "- Tune `concurrency` based on your workload\n",
    "- Aggregate results for bulk analysis\n",
    "- dppvalidator can handle 80k+ validations/second"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
